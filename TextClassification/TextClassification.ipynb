{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "duaFSdSdjydw"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "63Vq4fUYj4QL"
   },
   "outputs": [],
   "source": [
    "classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "3NgjFNyTkK8k"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I absolutely love this product; it exceeded all my expectations!\",  # Positive\n",
    "    \"The service was exceptional, and I would definitely recommend it.\",  # Positive\n",
    "    \"This feature has made my experience so much better!\",  # Positive\n",
    "    \"I am thoroughly impressed with the speed and functionality of this app.\",  # Positive\n",
    "    \"The food was delicious and the presentation was amazing!\",  # Positive\n",
    "    \"The event went as expected with no major issues.\",  # Neutral\n",
    "    \"It was neither good nor bad, just an average experience.\",  # Neutral\n",
    "    \"The instructions were clear, but nothing stood out.\",  # Neutral\n",
    "    \"The meeting was okay, but we didn't cover much new information.\",  # Neutral\n",
    "    \"The product works fine, but it doesn't bring anything extraordinary.\",  # Neutral\n",
    "    \"I’m really disappointed with the quality of the product.\",  # Negative\n",
    "    \"The service was poor and not worth the price.\",  # Negative\n",
    "    \"I regret purchasing this; it didn’t meet any of my expectations.\",  # Negative\n",
    "    \"The app crashes frequently, and it’s very frustrating.\",  # Negative\n",
    "    \"The delivery was delayed, and the packaging was damaged.\"  # Negative\n",
    "]\n",
    "\n",
    "positive_labels = [\n",
    "    'love', 'admiration', 'approval', 'gratitude', 'joy', 'optimism',\n",
    "    'desire', 'excitement', 'amusement', 'curiosity', 'pride', 'relief', 'caring'\n",
    "]\n",
    "\n",
    "negative_labels = [\n",
    "    'disappointment', 'disapproval', 'annoyance', 'anger', 'sadness', 'disgust',\n",
    "    'fear', 'remorse', 'embarrassment', 'nervousness', 'grief'\n",
    "]\n",
    "\n",
    "neutral_labels = ['neutral', 'surprise', 'realization', 'confusion']\n",
    "\n",
    "true_labels = [\n",
    "    \"POSITIVE\",\n",
    "    \"POSITIVE\",\n",
    "    \"POSITIVE\",\n",
    "    \"POSITIVE\",\n",
    "    \"POSITIVE\",\n",
    "    \"NEUTRAL\",\n",
    "    \"NEUTRAL\",\n",
    "    \"NEUTRAL\",\n",
    "    \"NEUTRAL\",\n",
    "    \"NEUTRAL\",\n",
    "    \"NEGATIVE\",\n",
    "    \"NEGATIVE\",\n",
    "    \"NEGATIVE\",\n",
    "    \"NEGATIVE\",\n",
    "    \"NEGATIVE\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGSRslk5uqhh",
    "outputId": "dc7a488c-70f4-41f1-9c32-fdc2d756c590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I absolutely love this product; it exceeded all my expectations! -> POSITIVE\n",
      "The service was exceptional, and I would definitely recommend it. -> POSITIVE\n",
      "This feature has made my experience so much better! -> POSITIVE\n",
      "I am thoroughly impressed with the speed and functionality of this app. -> POSITIVE\n",
      "The food was delicious and the presentation was amazing! -> POSITIVE\n",
      "The event went as expected with no major issues. -> POSITIVE\n",
      "It was neither good nor bad, just an average experience. -> POSITIVE\n",
      "The instructions were clear, but nothing stood out. -> NEUTRAL\n",
      "The meeting was okay, but we didn't cover much new information. -> POSITIVE\n",
      "The product works fine, but it doesn't bring anything extraordinary. -> NEUTRAL\n",
      "I’m really disappointed with the quality of the product. -> NEGATIVE\n",
      "The service was poor and not worth the price. -> NEGATIVE\n",
      "I regret purchasing this; it didn’t meet any of my expectations. -> NEGATIVE\n",
      "The app crashes frequently, and it’s very frustrating. -> NEGATIVE\n",
      "The delivery was delayed, and the packaging was damaged. -> NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "for i in sentences:\n",
    "  current = \"\"\n",
    "  model_outputs = classifier(i)\n",
    "  if model_outputs[0][0][\"label\"] in positive_labels:\n",
    "    current = \"POSITIVE\"\n",
    "  elif model_outputs[0][0][\"label\"] in negative_labels:\n",
    "    current = \"NEGATIVE\"\n",
    "  elif model_outputs[0][0][\"label\"] in neutral_labels:\n",
    "    current = \"NEUTRAL\"\n",
    "  print(f\"{i} -> {current}\")\n",
    "  predicted_labels.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "xQDQonYbpWSX"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WvyvbJ_r4eX",
    "outputId": "a1766d72-0614-4abd-9679-75dafaa186f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "Precision: 0.81\n",
      "Recall: 0.70\n",
      "F1-Score: 0.67\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [3 2]]\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro', labels=['POSITIVE', 'NEUTRAL'])\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "recall = recall_score(true_labels, predicted_labels, average='macro', labels=['POSITIVE', 'NEUTRAL'])\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro', labels=['POSITIVE', 'NEUTRAL'])\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels, labels=['POSITIVE', 'NEUTRAL'])\n",
    "print(f\"Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bShvHoDxdaB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
